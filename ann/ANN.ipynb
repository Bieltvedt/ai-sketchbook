{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from inspect import isfunction\n",
    "import inspect\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import heapq\n",
    "import logging\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure the logger\n",
    "logger = logging.getLogger('loggy')\n",
    "\n",
    "if not logger.handlers:\n",
    "    # Execute this only once to avoid adding multiple handlers\n",
    "    logger.setLevel(logging.WARNING)  # Default level\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# debug -> info -> warning -> error -> critical\n",
    "def update_logger_level(new_level):\n",
    "    \"\"\"Updates the logger's level.\"\"\"\n",
    "    level = getattr(logging, new_level.upper(), None)\n",
    "    if level is not None:\n",
    "        logger.setLevel(level)\n",
    "    else:\n",
    "        logger.warning(\"Invalid logging level:\", new_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.genfromtxt(\"./data/features.txt\", delimiter=\",\")\n",
    "targets = np.genfromtxt(\"./data/targets.txt\", delimiter=\",\")\n",
    "unknown = np.genfromtxt(\"./data/unknown.txt\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_split(features, targets, test_size=0.1, validation_size=0.1, random_state=None):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = np.arange(len(features))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Determine sizes of train, validation, and test sets\n",
    "    num_samples = len(features)\n",
    "    num_validation = int(num_samples * validation_size)\n",
    "    num_test = int(num_samples * test_size)\n",
    "    num_train = num_samples - num_validation - num_test\n",
    "\n",
    "    # Split indices\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_validation]\n",
    "    test_indices = indices[num_train + num_validation:]\n",
    "\n",
    "    # Split data based on indices\n",
    "    features_train = features[train_indices]\n",
    "    features_val = features[val_indices]\n",
    "    features_test = features[test_indices]\n",
    "    targets_train = targets[train_indices]\n",
    "    targets_val = targets[val_indices]\n",
    "    targets_test = targets[test_indices]\n",
    "\n",
    "    return features_train, features_val, features_test, targets_train, targets_val, targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train, features_val, features_test, targets_train, targets_val, targets_test = custom_split(features, targets, test_size=0.1, validation_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6284,)\n"
     ]
    }
   ],
   "source": [
    "print(targets_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom MLP implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mlp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mlp_func:\n",
    "    \"\"\"\n",
    "    This class stores functions for activation functions, loss functions, and their derivatives.\n",
    "\n",
    "    Also includes some shared functionality.\n",
    "\n",
    "    The class is accessed:\n",
    "        func.SIGMOID (func.FUNCTION_NAME)\n",
    "    \"\"\"\n",
    "\n",
    "    ## ACTIVATIONS\n",
    "    SIGMOID = staticmethod(lambda x: 1 / (1 + np.exp(-x)))\n",
    "    DSIGMOID = staticmethod(lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x)))))\n",
    "\n",
    "    TANH = staticmethod(lambda x: np.tanh(x))\n",
    "    DTANH = staticmethod(lambda x: 1 - np.power(np.tanh(x), 2))\n",
    "\n",
    "    ReLU = staticmethod(lambda x: np.where(x > 0, x, 0))\n",
    "    DReLU = staticmethod(lambda x: np.where(x > 0, 1, 0))\n",
    "\n",
    "    LReLU = staticmethod(lambda x, eps=1e-5: np.where(x > 0, x, x*eps))\n",
    "    DLReLU = staticmethod(lambda x, eps=1e-5: np.where(x > 0, 1, eps))\n",
    "\n",
    "    SOFTMAX = staticmethod(lambda x: np.exp(x - np.max(x, axis=1, keepdims=True)) / np.sum(np.exp(x - np.max(x, axis=1, keepdims=True)), axis=1, keepdims=True))\n",
    "    # No softmax derivitive implementation, only the \"cheat\" one with cross entropy\n",
    "\n",
    "    REGRESSION = staticmethod(lambda x: x)\n",
    "    DREGRESSION = staticmethod(lambda x: 1)\n",
    "\n",
    "\n",
    "    ## LOSS FUNCTIONS\n",
    "    MSE = staticmethod(lambda y_true, y_pred: np.mean(np.power((y_true - y_pred), 2)))\n",
    "    DMSE = staticmethod(lambda y_true, y_pred: (y_pred - y_true) / y_true.shape[0])\n",
    "\n",
    "    CROSS_ENTROPY = staticmethod(lambda y_true, y_pred, eps=1e-15: -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1)))\n",
    "    DCROSS_ENTROPY = staticmethod(lambda y_true, y_pred: y_pred - y_true)\n",
    "\n",
    "\n",
    "    ## REGULARIZATION\n",
    "    L2REG = staticmethod(lambda weights, lambda_reg: 2 * lambda_reg * weights)\n",
    "    L1REG = staticmethod(lambda weights, lambda_reg: np.sign(weights) * lambda_reg)\n",
    "    NONE = staticmethod(lambda weights, lambda_reg: np.zeros_like(weights))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def validHiddenActivation(cls, function) -> bool:\n",
    "        \"\"\"Checks if function is a valid, predefined, hidden activation\"\"\"\n",
    "        return function in [cls.SIGMOID, cls.TANH, cls.ReLU, cls.LReLU]\n",
    "    \n",
    "    @classmethod\n",
    "    def validOutputActivation(cls, function) -> bool:\n",
    "        \"\"\"Checks if function is a valid, predefined, output activation\"\"\"\n",
    "        return function in [cls.SIGMOID, cls.SOFTMAX, cls.REGRESSION]\n",
    "    \n",
    "    @classmethod\n",
    "    def validLossFunction(cls, function) -> bool:\n",
    "        \"\"\"Checks if function is a valid, predefined, loss function\"\"\"\n",
    "        return function in [cls.MSE, cls.CROSS_ENTROPY]\n",
    "    \n",
    "    @classmethod\n",
    "    def validRegFunction(cls, function) -> bool:\n",
    "        \"\"\"Checks if function is a valid, predefined, regularization function\"\"\"\n",
    "        return function in [cls.NONE, cls.L1REG, cls.L2REG]\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_custom(cls, function, role):\n",
    "        \"\"\"\n",
    "        Validates custom function\n",
    "        Checks args and output shape\n",
    "\n",
    "            NOTE : No implementation for custom dervitives, so custom functions dont work!\n",
    "\n",
    "            Args: \n",
    "                function: function to check\n",
    "                role: role the function will fufill \n",
    "        \"\"\"\n",
    "        if not isfunction(function):\n",
    "            raise ValueError(f\"Custom {role} function must be callable, was {function.type()}\")\n",
    "        \n",
    "        arg_spec = inspect.getfullargspec(function)\n",
    "        args = arg_spec.args\n",
    "        defaults = arg_spec.defaults if arg_spec.defaults is not None else []\n",
    "        \n",
    "        # Expected arguments\n",
    "        expected_args = {\n",
    "            \"loss\": [\"y_true\", \"y_pred\", \"eps\"],\n",
    "            \"output\": [\"x\", \"eps\"],\n",
    "            \"hidden\": [\"x\", \"eps\"],\n",
    "            \"regularization\": [\"weights\", \"lambda_reg\", \"eps\"]\n",
    "        }\n",
    "        \n",
    "        # Validate args\n",
    "        if role not in expected_args:\n",
    "            raise ValueError(f\"Invalid function type: {role}. Must be one of {list(expected_args.keys())}\")\n",
    "        \n",
    "        missing_args = [arg for arg in expected_args[role] if arg != 'eps' and arg not in args]\n",
    "        if missing_args:\n",
    "            raise ValueError(f\"Missing required arguments for {role} function: {missing_args}\")\n",
    "        \n",
    "        unexpected_args = [arg for arg in args if arg not in expected_args[role]]\n",
    "        if unexpected_args:\n",
    "            raise ValueError(f\"Unexpected arguments for {role} function: {unexpected_args}\")\n",
    "        \n",
    "\n",
    "        # Check for epsilon default value if eps is expected, must have a default value\n",
    "        if 'eps' in expected_args[role] and 'eps' in args:\n",
    "            eps_index = args.index('eps') - (len(args) - len(defaults)) \n",
    "            if eps_index >= 0:\n",
    "                eps_default = defaults[eps_index]\n",
    "                if eps_default is None:\n",
    "                    raise ValueError(\"Epsilon parameter must have a default value\")\n",
    "            else:\n",
    "                # Epsilon is expected but no default value is provided\n",
    "                raise ValueError(\"Epsilon parameter is expected to have a default value but none was provided\")\n",
    "\n",
    "\n",
    "        # validate shape (can be changed later)\n",
    "        dummy_input_shapes = {\n",
    "            \"loss\": {\"shapes\": [(10, 1), (10, 1)], \"validation\":cls._validate_shape_loss},\n",
    "            \"output\": {\"shapes\": (25, 50), \"validation\":cls._validate_shape_actiavation}, \n",
    "            \"input\": {\"shapes\": (25, 50), \"validation\":cls._validate_shape_actiavation}, \n",
    "            \"regularization\": {\"shapes\": (100, 125), \"validation\":cls._validate_shape_regularization}\n",
    "        }\n",
    "\n",
    "        dummy_input_shapes[role][\"validation\"](function, dummy_input_shapes[role][\"shapes\"])\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def _validate_shape_loss(cls, function, shapes):\n",
    "        \"\"\"Validates shape for loss function\"\"\"\n",
    "        dummy_y_true = np.random.rand(*shapes[0])\n",
    "        dummy_y_pred = np.random.rand(*shapes[1])\n",
    "        output = function(dummy_y_true, dummy_y_pred)\n",
    "\n",
    "        if not np.isscalar(output) and output.shape != (shapes[0][0], 1):\n",
    "            raise ValueError(f\"loss function shape was unexpected, should be scalar or {(shapes[0][0], 1)}, was {output.shape}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def _validate_shape_actiavation(cls, function, input_shape):\n",
    "        \"\"\"Validates shape for activation function\"\"\"\n",
    "        dummy_input = np.random.rand(*input_shape)\n",
    "        output = function(dummy_input)\n",
    "\n",
    "        if not output.shape == input_shape:\n",
    "            raise ValueError(f\"activation function shape was unexpected. Should be {input_shape}, was {output.shape}\")\n",
    "        \n",
    "    @classmethod\n",
    "    def _validate_shape_regularization(cls, function, weights_shape):\n",
    "        \"\"\"Validates shape for regularization function\"\"\"\n",
    "        dummy_weights = np.random.rand(*weights_shape)\n",
    "        dummy_lambda = 0.001\n",
    "        output = function(dummy_weights, dummy_lambda)\n",
    "\n",
    "        if not output.shape == weights_shape:\n",
    "            raise ValueError(f\"regularization function shape was unexpected. Should be {weights_shape}, was {output.shape}\")\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def derv(cls, function):\n",
    "        \"\"\"\n",
    "        Retrieves the derivative function of a given function.\n",
    "\n",
    "        Args:\n",
    "            function: The function to derivate\n",
    "\n",
    "        Returns:\n",
    "            The derivative function if it exists, otherwise raises a ValueError.\n",
    "\n",
    "        \"\"\"\n",
    "        FUNC_TO_DERV = {cls.SIGMOID:cls.DSIGMOID, cls.TANH:cls.DTANH, cls.ReLU:cls.DReLU, cls.LReLU:cls.DReLU,\n",
    "                        cls.REGRESSION:cls.DREGRESSION, cls.MSE:cls.DMSE, cls.CROSS_ENTROPY:cls.DCROSS_ENTROPY}\n",
    "\n",
    "        if function == cls.SOFTMAX:\n",
    "            raise ValueError(\"Softmax derivitive not implemented\")\n",
    "            \n",
    "        if (not function in FUNC_TO_DERV):\n",
    "            raise ValueError(f\"Derivative not found for function : {func_parser.convert_func(function)}\")\n",
    "        \n",
    "        return FUNC_TO_DERV[function]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### init functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class init_func:\n",
    "    \"\"\"\n",
    "    Class for initializing weights for a layer\n",
    "    \"\"\"\n",
    "\n",
    "    ## Initialization strategies\n",
    "    RANDOM = staticmethod(lambda input_size, layer_size: np.random.rand(input_size, layer_size))\n",
    "    GUASSIAN = staticmethod(lambda input_size, layer_size: np.random.normal(0, 1, (input_size, layer_size)))\n",
    "    XAVIER = staticmethod(lambda input_size, layer_size: np.random.normal(0, np.sqrt(1 / input_size), (input_size, layer_size)))\n",
    "    HE = staticmethod(lambda input_size, layer_size: np.random.normal(0, np.sqrt(2. / input_size), (input_size, layer_size)))\n",
    "    \n",
    "    CONST_BIAS = staticmethod(lambda layer_size, const=0: np.ones(layer_size) * const)\n",
    "    XAVIER_BIAS = staticmethod(lambda layer_size, const=1: const * np.random.uniform(-np.sqrt(1. / layer_size), np.sqrt(1. / layer_size), (layer_size)))\n",
    "    ZERO_BIAS = staticmethod(lambda layer_size, const=None: np.zeros(layer_size))\n",
    "        \n",
    "    @classmethod\n",
    "    def validate_custom_init(cls, init_method, input_size, layer_size):\n",
    "        \"\"\"\n",
    "        validates custom initialization function\n",
    "        - is function\n",
    "        - takes 2 arguments\n",
    "        - return ndarray with shape (layer_size, input_size)\n",
    "\n",
    "        Args:\n",
    "            init_method: custom init function to use\n",
    "            input_size: size of the input (layer before)\n",
    "            layer_size: size of the current layer\n",
    "\n",
    "        Returns:\n",
    "            initialized ndarray (layer_size, input_size)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if not a valid initialization function\n",
    "        \"\"\"\n",
    "        if not isfunction(init_method):\n",
    "            raise ValueError(f\"custom initialization requires passing a custom function to init_method, was : {init_method.type()}\")\n",
    "        if init_method.__code__.co_argcount != 2: \n",
    "            raise ValueError(f\"custom initialization function must take 2 paramaters, has {init_method.__code__.co_argcount}\")\n",
    "        \n",
    "        w_test = init_method(input_size, layer_size)\n",
    "\n",
    "        if not isinstance(w_test, np.ndarray):\n",
    "            raise ValueError(f\"custom initialization function must return ndarray, was {w_test.type()}\")\n",
    "        if w_test.shape != (input_size, layer_size):\n",
    "            raise ValueError(f\"Custome initialize function must return ndarray with shape {(layer_size, input_size)}, was {w_test.shape()}\")\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def validInitFunc(cls, init_method):\n",
    "        \"\"\"\n",
    "        Validates initialization function\n",
    "\n",
    "        Args:\n",
    "            init_method: init function to validate\n",
    "\n",
    "        Raises:\n",
    "            ValueError: init_method is not a function or not in the list of valid functions\n",
    "        \"\"\" \n",
    "        VALID_INIT_FUNCS = [cls.RANDOM, cls.GUASSIAN, cls.XAVIER, cls.HE]\n",
    "\n",
    "        if not isfunction(init_method): \n",
    "            raise ValueError(f\"Init method did not evaluate to function, was {init_method.type()}\")\n",
    "        \n",
    "        if init_method in VALID_INIT_FUNCS: \n",
    "            return True\n",
    "        \n",
    "        if init_method not in VALID_INIT_FUNCS and cls.validate_custom_init(init_method):\n",
    "            return True\n",
    "        \n",
    "        raise ValueError(\"Invalid custom init passed, this message show never be shown\")\n",
    "        \n",
    "    @classmethod\n",
    "    def validBiasFunc(cls, init_bias):\n",
    "        VALID_BIAS = [cls.CONST_BIAS, cls.XAVIER_BIAS,init_func.ZERO_BIAS]\n",
    "        return init_bias in VALID_BIAS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### function parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class func_parser:\n",
    "    \"\"\"\n",
    "    Parses and validates functions for the mlp. Also handles custom function validation\n",
    "\n",
    "    Uses mlp_func and init_func\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ## Conversion dicts\n",
    "    STR_TO_FUNC = {\n",
    "        \"sigmoid\":mlp_func.SIGMOID, \"tanh\":mlp_func.TANH, \"relu\":mlp_func.ReLU, \n",
    "        \"lrelu\":mlp_func.LReLU, \"softmax\":mlp_func.SOFTMAX, 'regression':mlp_func.REGRESSION,\n",
    "        'mse':mlp_func.MSE, 'cross_entropy':mlp_func.CROSS_ENTROPY, \n",
    "        \"l2\":mlp_func.L2REG, 'l1': mlp_func.L1REG, 'none':mlp_func.NONE,\n",
    "        \"random\":init_func.RANDOM, \"guassian\":init_func.GUASSIAN, \n",
    "        \"xavier\":init_func.XAVIER, \"he\":init_func.HE, \"xavier_bias\":init_func.XAVIER_BIAS, \n",
    "        \"const_bias\":init_func.CONST_BIAS, \"zero_bias\":init_func.ZERO_BIAS\n",
    "    }\n",
    "    ROLE_TO_VALIDATION = {\n",
    "        \"hidden\":mlp_func.validHiddenActivation, \"output\":mlp_func.validOutputActivation, \n",
    "        \"loss\":mlp_func.validLossFunction, \"regularization\":mlp_func.validRegFunction,\n",
    "        \"init\":init_func.validInitFunc, \"bias\":init_func.validBiasFunc\n",
    "    }\n",
    "    ROLE_TO_FUNC = {\n",
    "        \"hidden\": [mlp_func.SIGMOID, mlp_func.TANH, mlp_func.ReLU, mlp_func.LReLU],\n",
    "        \"output\": [mlp_func.SIGMOID, mlp_func.SOFTMAX, mlp_func.REGRESSION],\n",
    "        \"loss\": [mlp_func.MSE, mlp_func.CROSS_ENTROPY],\n",
    "        \"regularization\": [mlp_func.NONE, mlp_func.L1REG, mlp_func.L2REG],\n",
    "        \"init\": [init_func.RANDOM, init_func.GUASSIAN, \n",
    "                 init_func.XAVIER, init_func.HE],\n",
    "        \"bias\": [init_func.XAVIER_BIAS, init_func.CONST_BIAS, init_func.ZERO_BIAS]\n",
    "    }\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def convert_and_validate(cls, function, role=None):\n",
    "        \"\"\"\n",
    "        Converts and validates input function\n",
    "\n",
    "        Args:\n",
    "            function: Input function. Can be valid function string, predefined function, or custom function\n",
    "            role: What role? [\"hidden\", \"output\", \"loss\", \"regularization\", \"init\"] default None and \n",
    "                auto inferrence, must pass explicit role if using a custom function.\n",
    "\n",
    "        Returns:\n",
    "            converted and validated function\n",
    "\n",
    "        Raises:\n",
    "            ValueError if function cannot be converted or if function or role invalid\n",
    "        \"\"\"\n",
    "        function = cls.convert(function)\n",
    "\n",
    "        if role == None:\n",
    "            role = cls._role_from_function(function)\n",
    "\n",
    "        valid = cls.validate(function, role)\n",
    "        if not valid:\n",
    "            raise ValueError(\"Invalid function! [This error should not be raised]\")\n",
    "        \n",
    "        return function\n",
    "\n",
    "    @classmethod\n",
    "    def convert(cls, function):\n",
    "        \"\"\"\n",
    "        Converts string to function if needed.\n",
    "\n",
    "        Args:\n",
    "            function: Function to be converted. String or callable (goes straight through)\n",
    "        \n",
    "        Returns:\n",
    "            valid function\n",
    "\n",
    "        Raises:\n",
    "            ValueError if function is not string or callable, or invalid function string.\n",
    "        \"\"\"\n",
    "        if isfunction(function):\n",
    "            return function\n",
    "        \n",
    "        elif isinstance(function, str):\n",
    "            function = function.lower()\n",
    "            if cls._valid_str_for_conversion(function):\n",
    "                return cls.STR_TO_FUNC[function]\n",
    "            raise ValueError(f\"Invalid function string: {function}\")\n",
    "        \n",
    "        raise ValueError(f\"function must be of type function or string, was {function.type()}\")\n",
    "\n",
    "    @classmethod\n",
    "    def validate(cls, function, role):\n",
    "        \"\"\"\n",
    "        Validates function for role\n",
    "        Checks if it is predefined or custom. Different validation\n",
    "\n",
    "        Args:\n",
    "            function: callable function\n",
    "            role: What role? [\"hidden\", \"output\", \"loss\", \"regularization\", \"init\"]\n",
    "\n",
    "        Returns:\n",
    "            Boolean indicating wether the function was valid, invalid functions should raise errors\n",
    "\n",
    "        Raises:\n",
    "            ValueError if function is not callable, invalid for the role, or fails custom checks.\n",
    "        \"\"\"\n",
    "        if not isfunction(function):\n",
    "            raise ValueError(f\"Function was not converted before validation!\") \n",
    "        \n",
    "        if cls._is_predifined_func(function) and not cls.ROLE_TO_VALIDATION[role](function):\n",
    "            raise ValueError(f\"Function {function} not valid for {role}\")\n",
    "        elif cls._is_predifined_func(function):\n",
    "            return cls.ROLE_TO_VALIDATION[role](function)\n",
    "        \n",
    "        if role == \"init\": \n",
    "            return init_func.validate_custom_init(function, 10, 20)\n",
    "        else: \n",
    "            return mlp_func.validate_custom(function, role)\n",
    "                \n",
    "\n",
    "    @classmethod\n",
    "    def _valid_str_for_conversion(cls, function: str):\n",
    "        \"\"\"\n",
    "        Checks wether a string is valid for conversion\n",
    "\n",
    "        Args:\n",
    "            function: function string\n",
    "\n",
    "        Returns:\n",
    "            Boolean indicating wether function string can be converted..\n",
    "        \"\"\"\n",
    "        if function in cls.STR_TO_FUNC.keys():\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    @classmethod\n",
    "    def _is_predifined_func(cls, function):\n",
    "        \"\"\"\n",
    "        Checks wether a function is a predefined one.\n",
    "\n",
    "        Args:\n",
    "            function: callable function to check\n",
    "\n",
    "        Returns:\n",
    "            boolean indicating wether function is predefined\n",
    "\n",
    "        Raises:\n",
    "            ValueError if function is not callable.\n",
    "        \"\"\"\n",
    "        if not isfunction(function):\n",
    "            raise ValueError(f\"_is_predifined_func only accepts functions, was {function.type()}\")\n",
    "        return function in cls.STR_TO_FUNC.values()\n",
    "    \n",
    "    @classmethod\n",
    "    def _find_key(cls, dict, val):\n",
    "        \"\"\"\n",
    "        Finds key in dictionary based on presense of value\n",
    "\n",
    "        Args:\n",
    "            dict: dict to check, has to have collection of values\n",
    "            val: val to look for\n",
    "\n",
    "        Returns:\n",
    "            first key that contains val in its values, None if none match\n",
    "        \"\"\"\n",
    "        for key, values in dict:\n",
    "            if val in values:\n",
    "                return key\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def _role_from_function(cls, function):\n",
    "        \"\"\"\n",
    "        Infers role from function, only works for predefined ones.\n",
    "\n",
    "        Args:\n",
    "            function: callable function\n",
    "\n",
    "        Returns:\n",
    "            role inferred from function\n",
    "\n",
    "        Raises:\n",
    "            ValueError if role could not be inferred or if function passed is not predefined.\n",
    "        \"\"\"\n",
    "        role = cls._find_key(cls.ROLE_TO_FUNC, function)\n",
    "        if role == None and cls._is_predifined_func(function):\n",
    "            raise ValueError(\"Role was not able to be inffered from function\")\n",
    "        elif role == None:\n",
    "            raise ValueError(\"Role can only be inffered from predefined functions, not custom ones\")\n",
    "        \n",
    "        return role\n",
    "    \n",
    "    @classmethod\n",
    "    def convert_func(cls, function):\n",
    "        \"\"\"Converts function to its string representation\"\"\"\n",
    "        if not isfunction(function):\n",
    "            raise ValueError(f\"convert_func must be called with function, was {function.type}\")\n",
    "\n",
    "        if function in cls.STR_TO_FUNC.values():\n",
    "            for key, value in cls.STR_TO_FUNC.items():\n",
    "                if value == function:\n",
    "                    return key\n",
    "                \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_helper:\n",
    "\n",
    "    logger = logging.getLogger(\"loggy\")\n",
    "\n",
    "    @classmethod\n",
    "    def convert_and_validate_functions(cls, activation_hidden, activation_output, loss_function, regularization, init_method, init_bias):\n",
    "        hidden = func_parser.convert_and_validate(activation_hidden, \"hidden\")\n",
    "        output = func_parser.convert_and_validate(activation_output, \"output\")\n",
    "        loss = func_parser.convert_and_validate(loss_function, \"loss\")\n",
    "        reg = func_parser.convert_and_validate(regularization, \"regularization\")\n",
    "        init = func_parser.convert_and_validate(init_method, \"init\")\n",
    "        bias = func_parser.convert_and_validate(init_bias, 'bias')\n",
    "\n",
    "        return hidden, output, loss, reg, init, bias\n",
    "    \n",
    "    @classmethod\n",
    "    def create_layers(cls, sizes, act_hidden, act_output, init, init_bias, bias_const, reg, reg_lambda):\n",
    "        layers = []\n",
    "        for i in range(0, len(sizes) - 2):\n",
    "            l = Layer(input_size=sizes[i], layer_size=sizes[i + 1], activation_func=act_hidden,\n",
    "                      init_method=init, init_bias=init_bias, bias_const=bias_const, regularization=reg, reg_lambda=reg_lambda)\n",
    "            layers.append(l)\n",
    "\n",
    "        output = Layer(input_size=sizes[-2], layer_size=sizes[-1], activation_func=act_output,\n",
    "                       init_method=init, init_bias=init_bias, bias_const=bias_const, regularization=reg, reg_lambda=reg_lambda)\n",
    "        \n",
    "        return layers + [output]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_subset(cls, input_data, target_data, subset_ratio = 0.2):\n",
    "        total_samples = input_data.shape[0]\n",
    "        subset_size = int(total_samples * subset_ratio)\n",
    "\n",
    "        # Generate random indices\n",
    "        indices = np.random.choice(total_samples, subset_size, replace=False)\n",
    "\n",
    "        subset_input_data = input_data[indices]\n",
    "        subset_target_data = target_data[indices]\n",
    "\n",
    "        return (subset_input_data, subset_target_data)\n",
    "    \n",
    "    @classmethod\n",
    "    def k_fold_split(cls, input_data, target_data, k_folds=5):\n",
    "        # Handle case where k < 2\n",
    "        if k_folds < 2:\n",
    "            logger.warning(f\"Attempted to split data into {k_folds}, returned NONE\")\n",
    "            return None\n",
    "            \n",
    "        # Shuffle indices\n",
    "        indices = np.arange(input_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Create folds\n",
    "        fold_sizes = input_data.shape[0] // k_folds\n",
    "        folds = []\n",
    "        for i in range(k_folds):\n",
    "            start = i * fold_sizes\n",
    "            end = (i + 1) * fold_sizes if i != k_folds-1 else input_data.shape[0]\n",
    "            test_indices = indices[start:end]\n",
    "            train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "            folds.append((input_data[train_indices], target_data[train_indices], input_data[test_indices], target_data[test_indices]))\n",
    "        return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "    @staticmethod\n",
    "    def min_max_scale(data):\n",
    "        df = data.copy()\n",
    "        mins = np.min(df, axis=0)  \n",
    "        maxs = np.max(df, axis=0)  \n",
    "        denom = maxs - mins  \n",
    "        denom[denom == 0] = 1  # Handle cases where max == min (set to 1 to avoid NaN)\n",
    "        return (df - mins) / denom\n",
    "    \n",
    "    @staticmethod\n",
    "    def z_score(data):\n",
    "        df = data.copy()\n",
    "        eps = 1e-8  \n",
    "        means = np.mean(df, axis=0)\n",
    "        stds = np.std(df, axis=0)\n",
    "        stds[stds == 0] = eps  # Handle cases where standard deviation is 0\n",
    "        return (df - means) / stds\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_shift(data):\n",
    "        df = data.copy()\n",
    "        scaled_df = df - df.mean()\n",
    "        return scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a single layer in an artificial neural network (ANN). This class can be extended to create different types of layers.\n",
    "    Each layer has an input size, layer size, activation function, and optional initialization method and regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, layer_size, activation_func, init_method = None, init_bias='const_bias', bias_const=0, regularization = None, reg_lambda = 0.01) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the layer with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vector.\n",
    "            layer_size (int): The number of neurons in the layer.\n",
    "            activation_func (callable): The activation function to use for neurons in the layer.\n",
    "            init_method (callable, optional): The method used to initialize the weights. Defaults to None.\n",
    "            regularization (callable, optional): The regularization function to apply to the weights. Defaults to None.\n",
    "            reg_lambda (float, optional): The regularization lambda parameter. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.layer_size = layer_size\n",
    "        self.activation_func = activation_func\n",
    "        self.init_method = init_method\n",
    "\n",
    "        self.freeze = False\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.x_values = []\n",
    "        self.logger = logging.getLogger('loggy')\n",
    "\n",
    "        self._init_regularization(regularization, reg_lambda)\n",
    "        \n",
    "        ## Initialize weights if init method available\n",
    "        if init_method: self.initialize_weights(init_method, init_bias, bias_const)\n",
    "\n",
    "    def _init_regularization(self, regularization, reg_lambda):\n",
    "        \"\"\"\n",
    "        Initializes the regularization method and lambda for the layer.\n",
    "\n",
    "        Args:\n",
    "            regularization (callable): The regularization function to apply to the weights.\n",
    "            reg_lambda (float): The regularization lambda parameter.\n",
    "        \"\"\"\n",
    "        if regularization == None: regularization = mlp_func.NONE\n",
    "            \n",
    "        self.regularization = regularization\n",
    "        self.reg_lambda = reg_lambda   \n",
    "\n",
    "    def _apply_regularization(self):\n",
    "        \"\"\"\n",
    "        Applies regularization to the layer's weights based on the defined regularization method and lambda.\n",
    "\n",
    "        Returns:\n",
    "            The regularization penalty to be applied to the weights.\n",
    "        \"\"\"\n",
    "        return self.regularization(self.weights, self.reg_lambda)  \n",
    "        \n",
    "    def initialize_weights(self, init_method, init_bias, bias_const):\n",
    "        \"\"\"\n",
    "        Initializes the layer's weights and biases using the specified initialization method.\n",
    "\n",
    "        Args:\n",
    "            init_method (callable): The method used to initialize the weights.\n",
    "        \"\"\"\n",
    "        self.weights = init_method(self.input_size, self.layer_size)\n",
    "        self.bias = init_bias(self.layer_size, bias_const)\n",
    "        self.logger.debug(f\"initial bias : {self.bias}\")\n",
    "        self.init_bias = init_bias\n",
    "        self.bias_const = bias_const\n",
    "        self.init_method = init_method\n",
    "\n",
    "        self.initial_weights = self.weights.copy()\n",
    "        self.initial_bias = self.bias.copy()\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the layer using the given input batch.\n",
    "        Called from MLP when preforming forward pass\n",
    "\n",
    "        Args:\n",
    "            input_batch (np.ndarray): The input data batch to process.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The output of the layer after applying the weights, biases, and activation function.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input batch size does not match the expected input size of the layer.\n",
    "        \"\"\"\n",
    "        if self.weights is None or not self.init_method:\n",
    "            raise ValueError(\"Layer weights have not been initialized!\")\n",
    "        \n",
    "        if input_batch.shape[1] != self.input_size:\n",
    "            raise ValueError(f\"Actual input size does not match defined, was {input_batch.shape[1]}, should be {self.input_size}\")\n",
    "\n",
    "        z = np.dot(input_batch, self.weights) + self.bias\n",
    "        x = self.activation_func(z)\n",
    "        \n",
    "        self.last_input = input_batch\n",
    "        self.x_values.append(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def calculate_gradient(self, output_error_batch):\n",
    "        \"\"\"\n",
    "        Calculates the gradients for the layer's weights and biases based on the output error.\n",
    "\n",
    "        Args:\n",
    "            output_error_batch (np.ndarray): The error between the predicted and actual outputs.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the gradients for the weights, biases, and the error to propagate to the previous layer.\n",
    "        \"\"\"\n",
    "        if self.activation_func != mlp_func.SOFTMAX:\n",
    "            # Ensure you're applying the derivative to the pre-activation values (z)\n",
    "            z = np.dot(self.last_input, self.weights) + self.bias\n",
    "            activation_derivative = mlp_func.derv(self.activation_func)(z)\n",
    "            delta = output_error_batch * activation_derivative\n",
    "    \n",
    "        else:\n",
    "            delta = output_error_batch  # For Softmax with cross-entropy, output_error_batch is already delta\n",
    "\n",
    "        # Calculate weight and bias gradient\n",
    "        weight_grad = np.dot(self.last_input.T, delta) / output_error_batch.shape[0]\n",
    "        bias_grad = np.sum(delta, axis=0) / output_error_batch.shape[0]\n",
    "\n",
    "        # Apply regularization\n",
    "        weight_grad += self.regularization(self.weights, self.reg_lambda)\n",
    "        \n",
    "        # Calculate error to propagate to next layer\n",
    "        error_to_propagate = np.dot(delta, self.weights.T)\n",
    "        \n",
    "        return weight_grad, bias_grad, error_to_propagate\n",
    "    \n",
    "    def update_weights(self, weight_grad, bias_grad, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the layer's weights and biases based on the calculated gradients and learning rate.\n",
    "\n",
    "        Args:\n",
    "            weight_grad (np.ndarray): The gradient of the weights.\n",
    "            bias_grad (np.ndarray): The gradient of the biases.\n",
    "            learning_rate (float): The learning rate to use for the update.\n",
    "        \"\"\"\n",
    "        if not self.freeze:\n",
    "            # Clip gradients\n",
    "            clipped_weight_grad = np.clip(weight_grad, -5, 5)\n",
    "            clipped_bias_grad = np.clip(bias_grad, -5, 5)\n",
    "            \n",
    "            # Update weights and biases with clipped gradients\n",
    "            self.weights -= learning_rate * clipped_weight_grad\n",
    "            self.bias -= learning_rate * clipped_bias_grad\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\" Prints information about the layer, including its configuration and current state. \"\"\"\n",
    "        print(f\"\"\"\n",
    "        Layer - \n",
    "            input size : {self.input_size}\n",
    "            layer size : {self.layer_size}\n",
    "            weights : {self.weights.shape}\n",
    "            \n",
    "            activation : {func_parser.convert_func(self.activation_func)}\n",
    "            regularization : {func_parser.convert_func(self.regularization)}\n",
    "            init : {func_parser.convert_func(self.init_method)}\n",
    "\n",
    "            Frozen : {self.freeze}\n",
    "        \"\"\")\n",
    "\n",
    "    def print_values(self):\n",
    "        \"\"\" \n",
    "        Prints the current weights, biases, last input, and last output of the layer, \n",
    "        as well as the initial weights and biases.\n",
    "        \"\"\"\n",
    "        print(f\"WEIGHTS\\n{self.weights}\")\n",
    "        print(f\"BIAS\\n{self.bias}\")\n",
    "        print(f\"LAST INPUT\\n{self.last_input}\")\n",
    "        print(f\"LAST OUTPUT\\n{self.x_values[-1]}\")\n",
    "        print(f\"INITIAL WEIGHTS\\n{self.init_weights}\")\n",
    "        print(f\"INITIAL BIAS\\n{self.init_bias}\")\n",
    "    \n",
    "    def freeze_parameters(self):\n",
    "        \"\"\" Freezes the layer's parameters, preventing them from being updated during training. \"\"\"\n",
    "        self.freeze = True\n",
    "\n",
    "    def defrost_parameters(self):\n",
    "        \"\"\" Unfreezes the layer's parameters, allowing them to be updated during training. \"\"\"\n",
    "        self.freeze = False\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\" Returns the history of outputs generated by the layer. \"\"\"\n",
    "        return self.x_values\n",
    "\n",
    "    def get_last_output(self):\n",
    "        \"\"\" Returns the last output produced by the layer. \"\"\"\n",
    "        return self.x_values[-1]\n",
    "    \n",
    "    def get_last_inputs(self):\n",
    "        \"\"\" Returns the last set of inputs processed by the layer. \"\"\"\n",
    "        return self.last_input\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\" Returns the current weights of the layer. \"\"\"\n",
    "        return self.weights\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "        Resets the layer's weights and biases to their initial values and clears the history \n",
    "        of inputs and outputs.\n",
    "        \"\"\"\n",
    "        self.weights = self.initial_weights\n",
    "        self.bias = self.initial_bias\n",
    "        self.freeze = False\n",
    "        self.x_values = []\n",
    "        self.last_input = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def _init_functions(self, hidden, output, loss, reg, init, init_bias):\n",
    "        hidden, output, loss, reg, init, init_bias = MLP_helper.convert_and_validate_functions(hidden, output, loss, reg, init, init_bias)\n",
    "        self.act_hidden = hidden\n",
    "        self.act_output = output\n",
    "        self.loss = loss\n",
    "        self.reg = reg\n",
    "        self.init = init\n",
    "        self.init_bias = init_bias\n",
    "\n",
    "    def _init_sizes(self, input_size, output_size, hidden_sizes):\n",
    "        self.input_size  = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "    def _init_layers(self, sizes, act_hidden, act_output, init, init_bias, bias_const, reg, reg_lambda):\n",
    "        layers = MLP_helper.create_layers(sizes, act_hidden, act_output, init, init_bias, bias_const, reg, reg_lambda)\n",
    "        self.layers = layers\n",
    "            \n",
    "\n",
    "    def __init__(\n",
    "            self, input_size, output_size, hidden_sizes = [], \n",
    "            activation_function_hidden = \"sigmoid\", activation_function_output = \"sigmoid\", \n",
    "            init_method = \"He\", init_bias = \"const_bias\", bias_const=0, regularization = 'none', reg_lambda = 0.01, \n",
    "            loss_function=\"mse\", learning_rate=0.01, epochs=1000, decay_rate = None,\n",
    "            logging_level = \"warning\"):\n",
    "        \"\"\"\n",
    "        Initializes an MLP instance.\n",
    "\n",
    "        Args:\n",
    "            input_size: Number of input features.\n",
    "            output_size: Number of output neurons.\n",
    "            hidden_sizes: List of sizes for hidden layers.\n",
    "\n",
    "            activation_function_hidden: Activation function for hidden layers.\n",
    "            activation_function_output: Activation function for output layer.\n",
    "            loss_function: Loss function for training ('mse', 'cross_entropy')\n",
    "            init_method: Weight initialization method ('random', 'Gaussian', 'Xavier', 'He')\n",
    "            regularization: regularization method (l1, l2, none)\n",
    "\n",
    "            reg lambda: controlls amount of regularizarion\n",
    "            learning_rate: Learning rate for gradient descent.\n",
    "            epochs: Number of training epochs.\n",
    "            decay_rate : for learning rate decrease. learning_rate = learning_rate * (1 - decay_rate * epoch)\n",
    "\n",
    "            Dervitives for custom funcs!!!\n",
    "        \"\"\"\n",
    "        # Initialize \"basic\" values\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epochs = epochs\n",
    "        self.bias_const = bias_const\n",
    "\n",
    "        self.x_values = []\n",
    "        self.validation = []\n",
    "        self.training_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "        self.y_true = None\n",
    "        self.y_pred = None\n",
    "        self.stop_training = False\n",
    "\n",
    "        # logging\n",
    "        self.logger = logging.getLogger(\"loggy\")\n",
    "        self.prev_logging_level = logging.getLevelName(self.logger.level)\n",
    "        update_logger_level(logging_level)\n",
    "\n",
    "        # Initialize and validate functions\n",
    "        self._init_functions(activation_function_hidden, activation_function_output, loss_function, regularization, init_method, init_bias)\n",
    "\n",
    "        # Initialize sizes \n",
    "        self._init_sizes(input_size, output_size, hidden_sizes)\n",
    "        \n",
    "        # create layers with self.top_models.append((model_instance, model_params, val_accuracy))\n",
    "        self._init_layers(self.sizes, self.act_hidden, self.act_output, self.init, self.init_bias, self.bias_const, self.reg, self.reg_lambda)\n",
    "        self.logger.debug(f\"MLP intialized\\t--\\tlayers : {self.sizes}\")\n",
    "\n",
    "    def activation_function_hidden_f(self, x, eps = None):\n",
    "        if eps:\n",
    "            return self.act_hidden(x, eps)\n",
    "        return self.act_hidden(x)\n",
    "    \n",
    "    def activation_function_output_f(self, x, eps=None):\n",
    "        if eps:\n",
    "            return self.act_output(x, eps)\n",
    "        return self.act_output(x)\n",
    "    \n",
    "    def loss_function_f(self, y_true, y_pred, eps=None):\n",
    "        if self.loss == mlp_func.CROSS_ENTROPY and self.act_output == mlp_func.SOFTMAX:\n",
    "            y_true = self._ensure_ohe(y_true)\n",
    "        \n",
    "        if eps:\n",
    "            return self.loss(y_true, y_pred, eps)\n",
    "        return self.loss(y_true, y_pred)\n",
    "\n",
    "    def activation_function_hidden_derivative_f(self, x, eps = None):\n",
    "        if eps:\n",
    "            return mlp_func.derv(self.act_hidden)(x, eps)\n",
    "        return mlp_func.derv(self.act_hidden)(x)\n",
    "    \n",
    "    def activation_function_output_derivative_f(self, x, eps = None):       \n",
    "        # Should never be called for softmax, derv not implemented! \n",
    "        if eps:\n",
    "            return mlp_func.derv(self.act_output)(x, eps)\n",
    "        \n",
    "        return mlp_func.derv(self.act_output)(x)\n",
    "    \n",
    "    def loss_function_derivative_f(self, y_true, y_pred, eps = None):\n",
    "        if self.loss == mlp_func.CROSS_ENTROPY and self.act_output == mlp_func.SOFTMAX:\n",
    "            y_true = self._ensure_ohe(y_true)\n",
    "\n",
    "        if eps:\n",
    "            return mlp_func.derv(self.loss)(y_true, y_pred, eps)\n",
    "        return mlp_func.derv(self.loss)(y_true, y_pred)\n",
    "    \n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        \"\"\"\n",
    "        Performs forwards pass on the network\n",
    "\n",
    "        Args:\n",
    "            input_batch: Input batch to work on, (batch_size, feature_size)\n",
    "        \n",
    "        Returns:\n",
    "            final layer output\n",
    "        \"\"\"\n",
    "        self.input = input_batch\n",
    "        self.x_values = [input_batch]  # Store the initial input\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(self.x_values[-1])\n",
    "            self.x_values.append(x)\n",
    "\n",
    "        # Set predicted labels\n",
    "        self.y_pred = self.x_values[-1]\n",
    "        return self.x_values[-1]\n",
    "\n",
    "\n",
    "    def backward(self, y_true_batch, y_pred_batch, learning_rate):\n",
    "        gradients = []\n",
    "\n",
    "        cur_error = self._calculate_output_error(y_true_batch, y_pred_batch)\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            grad_w, grad_b, cur_error = self.layers[i].calculate_gradient(cur_error)\n",
    "            gradients.append({\"grad_w\":grad_w, \"grad_b\":grad_b})\n",
    "        gradients.reverse()\n",
    "        return gradients\n",
    "        \n",
    "    # Could be issues with this? what if softmax output and mse loss?\n",
    "    def _calculate_output_error(self, y_true_batch, y_pred_batch):\n",
    "        if self.act_output == mlp_func.SOFTMAX and self.loss == mlp_func.CROSS_ENTROPY:\n",
    "            output_error = y_pred_batch - self._ensure_ohe(y_true_batch)\n",
    "            \n",
    "        elif y_pred_batch.shape != y_true_batch.shape:\n",
    "            raise ValueError(f\"y_pred and y_true shape mismatch\\tpred:{y_pred_batch.shape} true:{y_true_batch.shape}\")\n",
    "        \n",
    "        else:\n",
    "            output_error = self.loss_function_derivative_f(y_true_batch, y_pred_batch)\n",
    "        \n",
    "        return output_error\n",
    "    \n",
    "    def _ensure_ohe(self, labels):\n",
    "        num_classes = self.output_size\n",
    "\n",
    "        if labels.ndim > 1:\n",
    "            labels = labels.flatten()\n",
    "            \n",
    "        if labels.dtype != int:\n",
    "            labels = labels.astype(int)\n",
    "            \n",
    "        one_hot_encoded = np.zeros((labels.size, num_classes))\n",
    "        \n",
    "        # Subtract 1 from labels to convert labels from 1-based to 0-based indexing\n",
    "        labels_zero_based = labels - 1\n",
    "        \n",
    "        # Use np.arange to generate row indices and labels_zero_based for column indices\n",
    "        one_hot_encoded[np.arange(labels.size), labels_zero_based] = 1\n",
    "\n",
    "        assert self._test_ohe(one_hot_encoded, labels)\n",
    "        \n",
    "        return one_hot_encoded\n",
    "    \n",
    "    def _test_ohe(self, ohe, labels):\n",
    "        return np.sum(np.argmax(ohe, axis=1) + 1 == labels) == labels.shape[0]\n",
    "    \n",
    "    def _get_learning_rate(self, learning_rate, epoch):\n",
    "        \"\"\"\n",
    "        This function implements a linear learning rate decay scheduler.\n",
    "\n",
    "        Args:\n",
    "            epoch: Current epoch number (int).\n",
    "            initial_lr: Initial learning rate (float).\n",
    "            decay_rate: Decay rate per epoch (float).\n",
    "\n",
    "        Returns:\n",
    "            Learning rate for the current epoch (float).\n",
    "        \"\"\"\n",
    "        if self.decay_rate == None:\n",
    "            return learning_rate\n",
    "        return learning_rate * (1 - self.decay_rate * epoch)\n",
    "\n",
    "\n",
    "    def train(\n",
    "            self, input, targets, learning_rate, batch_size = 1, \n",
    "            input_val = None, target_val = None, validate_n_epochs = 1, \n",
    "            accumulation_steps = 5,  early_stopping=True, patience=10,\n",
    "            acc_threshold = 0.98, loss_change_threshold = 0.001):\n",
    "        \"\"\"\n",
    "        Trains the MLP on the provided dataset using mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X: Input data, numpy array of shape (num_samples, num_features).\n",
    "            y: Labels, numpy array of shape (num_samples,) or (num_samples, num_outputs).\n",
    "            batch_size: Size of each mini-batch. Defaults to one\n",
    "            learning_rate: Learning rate for the gradient descent update rule.\n",
    "        \"\"\"\n",
    "        n_samples = input.shape[0]\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "        accumulated_gradients = [[] for _ in range(len(self.layers))]\n",
    "\n",
    "        # early stopping \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.stop_training:\n",
    "                break\n",
    "            # Decaying learning rate\n",
    "            learning_rate = self._get_learning_rate(learning_rate, epoch)\n",
    "            \n",
    "            # Shuffle data at beginning of each epoch (apperently helps)\n",
    "            indices = np.arange(n_samples)     \n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = input[indices]\n",
    "            y_shuffled = targets[indices] \n",
    "\n",
    "            # Train on batches\n",
    "            predictions = []\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # Accumulate gradients\n",
    "                batch_gradients, prediction_tuple = self._train_on_batch(X_batch, y_batch, learning_rate)\n",
    "                predictions.append(prediction_tuple)\n",
    "                for layer_idx in range(len(batch_gradients)):\n",
    "                    accumulated_gradients[layer_idx].append(batch_gradients[layer_idx])\n",
    "                \n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    # Update weights using accumulated gradients\n",
    "                    for layer_idx in range(len(self.layers)):\n",
    "                        total_grad_w = np.sum([grad[\"grad_w\"] for grad in accumulated_gradients[layer_idx]], axis=0)\n",
    "                        total_grad_b = np.sum([grad[\"grad_b\"] for grad in accumulated_gradients[layer_idx]], axis=0)\n",
    "                        \n",
    "                        self.layers[layer_idx].update_weights(total_grad_w, total_grad_b, learning_rate)\n",
    "                        accumulated_gradients[layer_idx] = []  # Reset accumulated gradients\n",
    "\n",
    "            # Early Stopping Check (inneffecient/unclean after adding accuracy saves)\n",
    "            if input_val is not None and target_val is not None:\n",
    "                val_acc = self.test(input=input_val, targets=target_val, batch_size=batch_size, ohe=False)\n",
    "                self.validation_accuracies.append(val_acc)\n",
    "                \n",
    "                if epoch % validate_n_epochs == 0:\n",
    "                    self._validate(input_val, target_val, input_val_acc=val_acc)\n",
    "                    self._early_stop(patience, acc_threshold, loss_change_threshold)\n",
    "\n",
    "            # Store training accuracy and log\n",
    "            train_acc = sum([x[0] for x in predictions]) / sum([x[1] for x in predictions])\n",
    "            self.training_accuracies.append(train_acc)\n",
    "\n",
    "            # Store validation accuracy\n",
    "            self.logger.debug(f\"Epoch {epoch}/{self.epochs} completed.\")\n",
    "\n",
    "    def _train_on_batch(self, x_batch, y_true_batch, learning_rate):\n",
    "        y_pred_batch = self.forward(x_batch)\n",
    "        gradients = self.backward(y_true_batch, y_pred_batch, learning_rate)\n",
    "        train_acc, correct_preds, total_preds  = self.test_batch(\n",
    "            input_batch=None, target_batch=y_true_batch, ohe=False, predictions_input=y_pred_batch)\n",
    "        return gradients, (correct_preds, total_preds)\n",
    "\n",
    "\n",
    "    def _validate(self, input_val, target_val, input_val_acc = None):\n",
    "        targets_pred_val = self.predict(input_val)\n",
    "        val_loss = self.loss_function_f(target_val, targets_pred_val)\n",
    "        val_acc = self.test(input = input_val, targets=target_val) if input_val_acc is None else input_val_acc\n",
    "        self.validation.append((val_loss, val_acc))\n",
    "\n",
    "        self.logger.debug(f\"validation loss: {val_loss} \\tvalidation accuracy : {val_acc}\")\n",
    "\n",
    "    def _early_stop(self, patience, accuracy, threshold):\n",
    "        # check for increasing validation loss trend, indicating potential overfitting\n",
    "        if len(self.validation) > patience:\n",
    "            is_increasing = True  # assume the trend is increasing to start\n",
    "            for i in range(len(self.validation) - patience, len(self.validation) - 1):\n",
    "            # if the current loss is greater or equal to the next loss, trend is not strictly increasing\n",
    "                if self.validation[i][0] >= self.validation[i + 1][0]:\n",
    "                    is_increasing = False\n",
    "                    break\n",
    "            if is_increasing:\n",
    "                self.logger.debug(\"Validation loss has been increasing for a while, indicating the model has started to overfit, stopping the training.\")\n",
    "                self.stop_training = True\n",
    "                return\n",
    "        # stop training if the target accuracy has been reached on the validation set\n",
    "        if self.validation[-1][1] >= accuracy:\n",
    "            self.logger.debug(f\"The target accuracy on a validation set of {accuracy * 100:.2f}% has been achieved, stopping the training.\")\n",
    "            self.stop_training = True\n",
    "            return\n",
    "        # stop training if the reduction in validation loss falls below a specified threshold\n",
    "        if len(self.validation) >= patience:\n",
    "            flag = True\n",
    "            for i in range(patience - 1):\n",
    "                if self.validation[-2 - i][0] - self.validation[-1 - i][0] > threshold: # previous loss is bigger than the next one by more than threshold\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                self.logger.debug(f\"The change in validation loss stopped exceeding {threshold}, stopping training.\")\n",
    "                self.stop_training = True\n",
    "\n",
    "            \n",
    "            \n",
    "    def predict(self, input_batch):\n",
    "        \"\"\"Predicts outputs for a batch of input samples.\"\"\"\n",
    "        self.predictions = [input_batch]\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(self.predictions[-1])\n",
    "            self.predictions.append(x) \n",
    "            \n",
    "        return self.predictions[-1]\n",
    "    \n",
    "    def test(self, input, targets, batch_size = 10, ohe = False):\n",
    "        n_samples = targets.shape[0]\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "        total_correct_pred = 0\n",
    "        total_pred = 0\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            input_batch = input[start_idx:end_idx]\n",
    "            target_batch = targets[start_idx:end_idx]\n",
    "\n",
    "            batch_accuracy, correct_pred, n_pred = self.test_batch(input_batch, target_batch, ohe)\n",
    "            total_correct_pred += correct_pred\n",
    "            total_pred += n_pred\n",
    "        \n",
    "        accuracy = total_correct_pred / total_pred\n",
    "        self.logger.debug(f\"Accuracy : {accuracy}\\t|\\t{total_correct_pred} / {total_pred}\")\n",
    "        return accuracy\n",
    "\n",
    "    def test_batch(self, input_batch, target_batch, ohe=False, predictions_input=None):\n",
    "        if predictions_input is None:\n",
    "            predictions = self.predict(input_batch)\n",
    "        else:\n",
    "            predictions = predictions_input\n",
    "\n",
    "        if ohe and self.act_output == mlp_func.SOFTMAX:\n",
    "            predicted_labels = np.argmax(predictions, axis=1) + 1\n",
    "            target_labels = np.argmax(target_batch, axis=1) + 1\n",
    "\n",
    "        elif self.act_output == mlp_func.SOFTMAX:\n",
    "            # assumes target labels are not ohe\n",
    "            predicted_labels = np.argmax(predictions, axis=1) + 1\n",
    "            target_labels = target_batch\n",
    "        else:\n",
    "            predicted_labels = predictions\n",
    "            \n",
    "            \n",
    "        predicted_labels = np.squeeze(predicted_labels)  # Reduce to 1D if not already\n",
    "        target_labels = np.squeeze(target_labels)  # Ensure target_labels is also 1D\n",
    "            \n",
    "        correct_predictions = np.sum(predicted_labels == target_labels)\n",
    "        \n",
    "        self.logger.debug(f\"correct preds : {correct_predictions} / {target_batch.shape[0]}\")\n",
    "        \n",
    "        accuracy = correct_predictions / target_batch.shape[0]\n",
    "        return accuracy, correct_predictions, target_batch.shape[0]\n",
    "\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Prints the current weights, biases, and intermediate outputs of the network.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"Layer {i}:\")\n",
    "            layer.info()\n",
    "            layer.print_values()\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def get_accuracies(self):\n",
    "        \"\"\"Returns accuracies (train_accs, val_accs). accs are saved every n epochs (variable)\"\"\"\n",
    "        return self.training_accuracies, self.validation_accuracies\n",
    "    def get_epochs(self):\n",
    "        return self.epochs\n",
    "    def set_epochs(self, epochs):\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def reset_logger(self):\n",
    "        update_logger_level(self.prev_logging_level)\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_values = []\n",
    "        self.validation = []\n",
    "        self.training_accuracies = []\n",
    "        self.validation_accuracies = []\n",
    "        self.y_true = None\n",
    "        self.y_pred = None\n",
    "        self.stop_training = False\n",
    "        self.predictions = []\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mlp = (Normalization.z_score(features_train), targets_train.copy())\n",
    "val_mlp = (Normalization.z_score(features_val), targets_val.copy())\n",
    "test_mlp = (Normalization.z_score(features_test), targets_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = MLP(\n",
    "        input_size = 10, output_size = 7, hidden_sizes = [14], \n",
    "        activation_function_hidden = \"lrelu\", activation_function_output = \"softmax\",\n",
    "        init_method = \"he\", init_bias='xavier_bias', bias_const = 1, regularization=\"l2\", reg_lambda=0.01,\n",
    "        loss_function = \"cross_entropy\", learning_rate=0.1, epochs=50, decay_rate=0.001, logging_level='info'\n",
    ")\n",
    "\n",
    "training_params_example = {\n",
    "    \"learning_rate\":0.1,\n",
    "    \"validate_n_epochs\":1,\n",
    "    \"accumulation_steps\":5,\n",
    "    \"early_stopping\":True,\n",
    "    \"patience\":10,\n",
    "    \"acc_threshold\":0.97,\n",
    "    \"loss_change_threshold\":1e-5,\n",
    "    'batch_size':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.train(\n",
    "    input=train_mlp[0], targets=train_mlp[1], input_val=val_mlp[0], target_val=val_mlp[1],\n",
    "    **training_params_example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model acc : 0.9248407643312102\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Cool new feature! - model stores training accuracy per epoch and validation accuracy per\n",
    "print(f\"model acc : {simple_model.test(input=test_mlp[0], targets=test_mlp[1])}\")\n",
    "train_accs, val_accs = simple_model.get_accuracies()\n",
    "epochs = range(simple_model.get_epochs())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, train_accs, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accs, label='Validation Accuracy')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs. Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridSearchOptimizer:\n",
    "    def _generate_combinations(self):\n",
    "        \"\"\"\n",
    "        Generates all combinations of hyperparameter values.\n",
    "        \"\"\"\n",
    "        keys, values = zip(*self.param_grid.items())\n",
    "        combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "        return combinations\n",
    "\n",
    "    def _generate_hidden_sizes(self, min_layers, max_layers, neuron_options):\n",
    "        \"\"\"\n",
    "        Generates a list of hidden layer sizes based on specified neuron options for each layer.\n",
    "        \n",
    "        Args:\n",
    "            min_layers (int): Minimum number of layers.\n",
    "            max_layers (int): Maximum number of layers.\n",
    "            neuron_options (list): List of neuron numbers for each layer to choose from.\n",
    "            \n",
    "        Returns:\n",
    "            list: A list of lists, where each list represents a possible combination of neuron counts.\n",
    "        \"\"\"\n",
    "        sizes = []\n",
    "        for num_layers in range(min_layers, max_layers + 1):\n",
    "            for combination in itertools.product(neuron_options, repeat=num_layers):\n",
    "                sizes.append(list(combination))\n",
    "        return sizes\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, param_grid, train_data, val_data, batch_sizes, fixed_params, training_params, ohe=False, top_k = 5, logging_level=\"info\"):\n",
    "        \"\"\"\n",
    "        Initializes the Grid Search Optimizer.\n",
    "        \n",
    "        Args:\n",
    "            model: A class representing your model, which should have a fit and evaluate method.\n",
    "            param_grid: Dictionary with parameters names (str) as keys and lists of parameter settings to try as values.\n",
    "            train_data: Tuple containing training data (input_train, target_train).\n",
    "            val_data: Tuple containing validation data (input_val, target_val).\n",
    "            batch_sizes: List of batch sizes to try\n",
    "            fixed_params: Dict with param names (str) as keys and fixed param values\n",
    "            training_params: Dict with training param names as keys and fixed param values\n",
    "            ohe : boolean representing wether targets are one hot encoded\n",
    "            top_k: top #k models to store at each time. Set to GridOptimizer.estimate_combinations to keep all\n",
    "            logging_level: sets logging level for logger, reset when finished optimizing\n",
    "\n",
    "            Note: \n",
    "            params not included in param_grid, fixed_params, or training_params will evaluate\n",
    "            to their default values\n",
    "        \"\"\"\n",
    "        # Set model and param dicts\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid.copy()\n",
    "        self.fixed_params = fixed_params.copy()\n",
    "        self.training_params = training_params.copy()\n",
    "\n",
    "        # Set data\n",
    "        self.train_data = (train_data[0].copy(), train_data[1].copy())\n",
    "        self.val_data = (val_data[0].copy(), val_data[1].copy())\n",
    "        self.batch_sizes = batch_sizes\n",
    "\n",
    "        # Initialize class params\n",
    "        self.ohe = ohe\n",
    "        self.top_k = top_k\n",
    "        self.top_models = []\n",
    "        self.top_k_models = []\n",
    "        self.counter = 0\n",
    "\n",
    "        # Setup logger\n",
    "        self.logger = logging.getLogger(\"loggy\")\n",
    "        self.prev_logging_level = logging.getLevelName(self.logger.level)\n",
    "        update_logger_level(logging_level)\n",
    "        \n",
    "        # Setup hidden sizes and combinations\n",
    "        self.param_grid['hidden_sizes'] = self._generate_hidden_sizes(*param_grid['hidden_sizes'])\n",
    "        self.combinations = self._generate_combinations()\n",
    "        self.n_combinations = len(self.combinations) * len(self.batch_sizes)\n",
    "        self.logger.info(\"OPTIMIZER INITIALIZED\")\n",
    "        self.logger.info(f\"N COMBINATIONS : {self.n_combinations}\\n\")\n",
    "\n",
    "    def optimize(self, k_folds = 5, use_subset=True, subset_ratio = 0.2, penalty_type = \"none\", penalty_factor = 0, stop_after_top_k = False, store_train_acc = False):\n",
    "        \"\"\"\n",
    "        Finds optimal hyperparameter combination\n",
    "\n",
    "            1. Finds top k preforming models (preferably on subset with k-fold cv) out of all possible\n",
    "                combinations of hyperparams\n",
    "            2. Resets, Trains, and Evaluates top k preforming models on full datasets\n",
    "            3. finds top k models, resets heap and logger level\n",
    "            \n",
    "\n",
    "            NOTE : this method can take really long to run, use estimate combinations to check if\n",
    "                it is feasible to run (each one will involve training a model)\n",
    "            NOTE : storing top k model instances is really expensive, dont make top_k too big\n",
    "            NOTE : get top model with get_top_model() and list of top k models with get_top_k_models()\n",
    "                Format - (acc, params, batch_size, model)\n",
    "\n",
    "            Args:\n",
    "                k_folds: #folds to use with k-fold cv in _find_top_k(), no k-fold cv if k_folds < 2\n",
    "                use_subset: boolean representing wether to use a subset in _find_top_k()\n",
    "                subset_ration: ratio of subset to original set\n",
    "                penalty_type: penalty type to use when calculating accuracy. ['none', 'abs', 'std', 'ratio']\n",
    "                penalty_factor: factor to apply to penalty before substracting, different depending on penalty_type\n",
    "                    automatically scaled for std : [-inf;0.05] -> [0.1;1] | [0.05;0.1] -> [1;5] | std ~ [0.1;inf] -> [5;10]\n",
    "                    f.x for std factor ~ [0.1, 1] (scaled), for ratio factor ~= 1/2\n",
    "                stop_after_top_k: stops after after _find_top_k\n",
    "                store_train_acc: store training accuracy\n",
    "        \"\"\"\n",
    "        # Store params\n",
    "        self.k_folds = k_folds\n",
    "        self.use_subset = use_subset\n",
    "        self.subset_ratio = subset_ratio\n",
    "        self.penalty_type = penalty_type\n",
    "        self.penalty_factor = penalty_factor\n",
    "        self.stop_after_top_k = stop_after_top_k\n",
    "        self.store_train_acc = store_train_acc\n",
    "\n",
    "        # Warning if not using subsets on large search space\n",
    "        if not use_subset and self.n_combinations > 100:\n",
    "            self.logger.warning(f\"Optimizing for {self.n_combinations} with full dataset!\")\n",
    "        \n",
    "        # Find top k on reduced dataset\n",
    "        self._find_top_k() \n",
    "\n",
    "        # Find best of top k models on full dataset\n",
    "        self._final_evaluate()\n",
    "\n",
    "        # reset logger level\n",
    "        update_logger_level(self.prev_logging_level)\n",
    "\n",
    "        # Output top model acc & params\n",
    "        model_info = self.get_top_model()\n",
    "        print(f\"\\tTOP MODEL\\t--\\tACCURACY : {model_info['val_acc']}\" + ('\\t\\t' + k + ':' + v) for k,v in model_info.items())\n",
    "\n",
    "\n",
    "    def _find_top_k(self):\n",
    "        \"\"\"\n",
    "        Executes the grid search optimization (preferably with k-fold CV and smaller subset)\n",
    "        Finds top k performing models\n",
    "        \"\"\"\n",
    "        # Setup\n",
    "        self.logger.info(f\"FIND TOP {self.top_k} MODELS\")\n",
    "        self.logger.info(f\"Fixed Training Parameters : {self.training_params}\")\n",
    "        self.logger.info(f\"Fixed Model Parameters : {self.fixed_params}\" + ('\\n' if self.k_folds > 1 else ''))\n",
    "        if self.k_folds < 2:\n",
    "            self.logger.info(f\"No k-fold cross validation!\\n\")\n",
    "        training_params = self.training_params.copy()\n",
    "\n",
    "        # Setup data (no copy could be bad?)\n",
    "        train_set = self.train_data\n",
    "        val_set = self.val_data\n",
    "\n",
    "        if self.use_subset:\n",
    "            train_set = MLP_helper.get_subset(train_set[0], train_set[1], self.subset_ratio)\n",
    "            val_set = MLP_helper.get_subset(val_set[0], val_set[1], self.subset_ratio)\n",
    "\n",
    "        folds = [(train_set + val_set)] if self.k_folds < 2 else MLP_helper.k_fold_split(k_folds=self.k_folds, *train_set)\n",
    "\n",
    "        # For progress bar\n",
    "        percentages = range(0, 101, 2)\n",
    "        previous_progress = set() \n",
    "        top_acc = - np.inf\n",
    "\n",
    "        # Iterate through all possible combinations\n",
    "        for combo in self.combinations:\n",
    "            # Create model instance with params\n",
    "            params = {**self.fixed_params, **combo}\n",
    "            model_instance = self.model(**params)\n",
    "\n",
    "            # Iterate through all possible batch sizes\n",
    "            for batch_size in self.batch_sizes:\n",
    "                # Progress bar implementation\n",
    "                progress = self.counter / self.n_combinations * 100\n",
    "                for threshold in percentages:\n",
    "                    if threshold <= progress and threshold not in previous_progress:\n",
    "                        previous_progress.add(threshold) \n",
    "                        logger.info(f\"Progress: {threshold}% completed ({self.counter} / {self.n_combinations})\\tbest accuracy : {top_acc}\")  \n",
    "\n",
    "                # Setup training params\n",
    "                training_params[\"learning_rate\"] = params[\"learning_rate\"]\n",
    "                training_params['batch_size'] = batch_size\n",
    "                \n",
    "                # Get accuracy and update top models\n",
    "                val_accs, train_accs = self._validate_k_folds(model_instance, training_params, folds)\n",
    "                accuracy = self._acc_with_penalty(val_accs, train_accs, self.penalty_type, self.penalty_factor)\n",
    "                top_acc = max(top_acc, accuracy)\n",
    "                \n",
    "                model_info = {'model':model_instance, 'params':params, 'val_acc':accuracy, 'batch_size':batch_size}\n",
    "                if self.store_train_acc: model_info['train_acc'] = np.mean(train_accs)\n",
    "                    \n",
    "                self._update_top_models(model_info)\n",
    "            \n",
    "                # Log and update counter\n",
    "                self.logger.debug(f'Training model with params: {combo} and batch_size: {batch_size}')\n",
    "                self.logger.debug(f\"Validation accuracy: {accuracy}\")\n",
    "                self.counter+=1\n",
    "        logger.info(f\"Progress: {percentages[-1]}% completed ({self.counter} / {self.n_combinations})\\tbest accuracy : {top_acc}\\n\") \n",
    "        gc.collect()\n",
    "    \n",
    "    def _final_evaluate(self):\n",
    "        \"\"\"\n",
    "        Performs final evaluation with top k models on full training set\n",
    "        Updates top models and extracts them to a list, resets heap\n",
    "        \"\"\"\n",
    "        # Setup \n",
    "        self.logger.info(\"FINAL EVALUATION\")\n",
    "        training_params = self.training_params.copy()\n",
    "        self.top_k_models = []\n",
    "        \n",
    "        while self.top_models:\n",
    "            _, _, model_info = heapq.heappop(self.top_models)\n",
    "            # setup params\n",
    "            training_params['batch_size'] = model_info['batch_size']\n",
    "            training_params['learning_rate'] = model_info['params']['learning_rate']\n",
    "            model_instance = model_info['model']\n",
    "\n",
    "            if not self.stop_after_top_k:\n",
    "                # Reset and retrain model on full datsets, evaluate on validation set\n",
    "                model_instance.reset()\n",
    "                model_instance.set_epochs(1000)\n",
    "                val_acc, train_acc = self._train_and_evaluate(model_instance, training_params, self.train_data, self.val_data)\n",
    "\n",
    "                # Set model info\n",
    "                model_info['val_acc'] = val_acc\n",
    "                if self.store_train_acc: model_info['train_acc'] = train_acc\n",
    "                \n",
    "                self.counter +=1\n",
    "                self.logger.info(\n",
    "                    f\"Model {self.counter - self.n_combinations} / {self.top_k}\\t accuracy = {val_acc}\" + \n",
    "                    ('\\n' if self.counter - self.n_combinations >= self.top_k - 1 else '')\n",
    "                )\n",
    "                gc.collect()\n",
    "            \n",
    "            # Update top models\n",
    "            self.top_k_models.append(model_info)\n",
    "\n",
    "        # convert top models to list, sort, and format\n",
    "        sorted_models = sorted(self.top_k_models, key=lambda x: x['val_acc'], reverse=True)\n",
    "        \n",
    "        # reset top models\n",
    "        self.top_models = []\n",
    "        self.top_k_models = sorted_models\n",
    "\n",
    "    def _train_and_evaluate(self, model_instance, training_params, train_data, val_data):\n",
    "        \"\"\"\n",
    "        Trains and evaluates model\n",
    "\n",
    "        Args:\n",
    "            model_instance : Initialzied model\n",
    "            training_params : dict of training params, expects all except data\n",
    "            input : tuple containing train and val input data\n",
    "            targets: tuple containing train and val target data\n",
    "\n",
    "        Returns:\n",
    "            val_acc: validation accuracy\n",
    "            train_acc: training accuracy\n",
    "        \"\"\"\n",
    "        # Unwrap data tuples\n",
    "        training_params['input'], training_params['targets'] = train_data\n",
    "        training_params['input_val'], training_params['target_val'] = val_data\n",
    "\n",
    "        # Train model instance\n",
    "        model_instance.train(**training_params)\n",
    "\n",
    "        # Get training and validation accuracy\n",
    "        val_acc = model_instance.test( \n",
    "            input = training_params['input_val'], targets = training_params['target_val'],\n",
    "            batch_size = training_params['batch_size'], ohe = self.ohe)\n",
    "        train_acc = model_instance.test(\n",
    "            input = training_params['input'], targets = training_params['targets'],\n",
    "            batch_size = training_params['batch_size'], ohe = self.ohe)\n",
    "        gc.collect()\n",
    "\n",
    "        return val_acc, train_acc\n",
    "\n",
    "    def _validate_k_folds(self, model_instance, training_params, folds):\n",
    "        \"\"\"\n",
    "        Validates model performance using k-fold cv\n",
    "        Sets up data (optional subset), accumulates accuracies over folds\n",
    "        If self.k_folds < 2 -> evaluates model on full train and validation sets\n",
    "\n",
    "        Args:\n",
    "            model_instance: model instance to validate\n",
    "            training_params: training params to use\n",
    "\n",
    "        Returns:\n",
    "            validation and training accuracy\n",
    "        \"\"\"\n",
    "        # accumulate accuracies\n",
    "        val_accs = []\n",
    "        train_accs = []\n",
    "        for input_train, targets_train, input_val, targets_val in folds:\n",
    "            val_acc, train_acc = self._train_and_evaluate(model_instance, training_params, (input_train, targets_train), (input_val, targets_val))\n",
    "            val_accs.append(val_acc)\n",
    "            train_accs.append(train_acc)\n",
    "        \n",
    "        return val_accs, train_accs\n",
    "\n",
    "    def _update_top_models(self, model_info):\n",
    "        \"\"\"\n",
    "        Updates the list of top models\n",
    "        Implemented with min heap, pushes untill k in heap, then pushes and pops smallest accuracy\n",
    "\n",
    "        Args:\n",
    "            model_info: val_acc, \n",
    "        \"\"\"\n",
    "        entry = (-model_info['val_acc'], self.counter, model_info)\n",
    "        \n",
    "        if len(self.top_models) < self.top_k:\n",
    "            heapq.heappush(self.top_models, entry)\n",
    "        else:\n",
    "            heapq.heappushpop(self.top_models, entry)\n",
    "\n",
    "    def _acc_with_penalty(self, val_accs, train_accs, penalty_type, penalty_factor):\n",
    "        \"\"\"\n",
    "        Adjusts accuracy with an overfitting penalty, ensuring penalty_factor scales within [0, 1].\n",
    "\n",
    "        Args:\n",
    "            val_accs: Validation accuracies as a numpy array or a scalar.\n",
    "            train_accs: Training accuracies as a numpy array or a scalar.\n",
    "\n",
    "        Returns:\n",
    "            Adjusted validation accuracy accounting for the penalty.\n",
    "        \"\"\"\n",
    "        val_accs = np.atleast_1d(val_accs)\n",
    "        train_accs = np.atleast_1d(train_accs)\n",
    "\n",
    "        val_acc_avg = np.mean(val_accs)\n",
    "\n",
    "        if penalty_type is None or penalty_factor <= 0 or penalty_type.lower() == \"none\":\n",
    "            return val_acc_avg\n",
    "\n",
    "        # Calculate average absolute difference between validation and training accuracies\n",
    "        avg_diff = np.mean(np.abs(val_accs - train_accs))\n",
    "\n",
    "        if penalty_type.lower() == \"abs\":\n",
    "            penalty = penalty_factor * avg_diff\n",
    "\n",
    "        elif penalty_type.lower() == \"std\":\n",
    "            std_diff = np.std(val_accs - train_accs)\n",
    "            penalty = penalty_factor * (1 / (1 + np.exp(-std_diff)))\n",
    "\n",
    "        elif penalty_type.lower() == \"ratio\":\n",
    "            ratio = avg_diff / val_acc_avg\n",
    "            penalty = penalty_factor * min(ratio, 1)\n",
    "\n",
    "        else:\n",
    "            self.logger.warning(f\"Unknown penalty type: {penalty_type}\")\n",
    "            return val_acc_avg\n",
    "\n",
    "        adjusted_acc = max(val_acc_avg - penalty, 0)\n",
    "        if adjusted_acc <= 1e-3: \n",
    "            self.logger.info(f\"Found overfit model : acc = {val_acc_avg}  penalty = {penalty}  avg diff = {avg_diff}\")\n",
    "        return adjusted_acc\n",
    "    \n",
    "    \n",
    "    def get_top_model(self):\n",
    "        \"\"\"\n",
    "        Returns the top model based on validation accuracy.\n",
    "        \"\"\"\n",
    "        if not self.top_k_models:\n",
    "            raise ValueError(f\"Optimization has not been run yet or no models were evaluated.\")\n",
    "        return self.top_k_models[0]\n",
    "\n",
    "    def get_top_k_models(self):\n",
    "        \"\"\"\n",
    "        Returns a list of the top k models sorted by validation accuracy.\n",
    "        \"\"\"\n",
    "        if not self.top_k_models:\n",
    "            raise ValueError(f\"Optimization has not been run yet or no models were evaluated.\")\n",
    "        return self.top_k_models\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def estimate_combinations(param_grid, batch_sizes):\n",
    "        \"\"\"\n",
    "        Estimates the total number of hyperparameter combinations, including different batch sizes.\n",
    "\n",
    "        Args:\n",
    "            param_grid (dict): The grid of parameters to search, with each key being a parameter name\n",
    "                               and each value being a list of values to try.\n",
    "            batch_sizes (list): List of batch sizes to try.\n",
    "\n",
    "        Returns:\n",
    "            int: The estimated number of combinations.\n",
    "        \"\"\"\n",
    "        total_combinations = 1  # Start with a base of 1\n",
    "\n",
    "        # Iterate through each parameter in the grid\n",
    "        for key, values in param_grid.items():\n",
    "            if key == 'hidden_sizes':\n",
    "                # Special handling for hidden_sizes\n",
    "                min_layers, max_layers, neuron_options = values\n",
    "                # Generate all possible hidden layer sizes and count them\n",
    "                layer_sizes_count = sum(len(neuron_options)**layers for layers in range(min_layers, max_layers + 1))\n",
    "                total_combinations *= layer_sizes_count\n",
    "            else:\n",
    "                total_combinations *= len(values)\n",
    "\n",
    "        # Multiply by the number of batch sizes\n",
    "        total_combinations *= len(batch_sizes)\n",
    "\n",
    "        return total_combinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
